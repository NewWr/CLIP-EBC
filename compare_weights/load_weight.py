weight_path = "/opt/DM/OCT/CLIP_Code/CLIP-EBC/compare_weights/open_clip_pytorch_model.bin"

import torch
import os
from collections import OrderedDict
import re

def load_weights(weight_path):
    """åŠ è½½æƒé‡æ–‡ä»¶"""
    if not os.path.exists(weight_path):
        print(f"âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")
        return None
    
    try:
        weights = torch.load(weight_path, map_location='cpu', weights_only=False)
        print(f"âœ… æˆåŠŸåŠ è½½: {weight_path}")
        return weights
    except Exception as e:
        print(f"âŒ åŠ è½½æƒé‡æ–‡ä»¶å¤±è´¥ {weight_path}: {e}")
        return None

def _extract_state_dict(weights):
    """ä»å¤šç§æ‰“åŒ…æ ¼å¼ä¸­æå– state_dict"""
    if isinstance(weights, dict):
        # å¸¸è§å­—æ®µä¼˜å…ˆ
        for key in ["state_dict", "model"]:
            if key in weights and isinstance(weights[key], dict):
                return weights[key]
        # æœ‰äº›ç›´æ¥å°±æ˜¯å‚æ•°å­—å…¸
        if all(isinstance(v, torch.Tensor) for v in weights.values()):
            return weights
        # å…œåº•ï¼šå°è¯•åœ¨ä¸€å±‚åµŒå¥—é‡Œæ‰¾
        for k, v in weights.items():
            if isinstance(v, dict) and all(isinstance(x, torch.Tensor) for x in v.values()):
                return v
    # è‹¥å·²æ˜¯ OrderedDict/å‚æ•°æ˜ å°„
    if isinstance(weights, (dict, OrderedDict)):
        return weights
    print("âš ï¸ æœªèƒ½ä»æƒé‡æ–‡ä»¶ä¸­æå–æœ‰æ•ˆçš„ state_dictã€‚")
    return None

def _strip_prefix(key):
    """å»é™¤å¸¸è§å‰ç¼€ï¼Œä¾‹å¦‚ 'module.'"""
    if key.startswith("module."):
        return key[len("module."):]
    return key

def _is_image_key(k):
    """åˆ¤å®šæ˜¯å¦ä¸ºå›¾åƒç¼–ç å™¨ç›¸å…³å‚æ•°"""
    # OpenCLIP/CLIP é€šå¸¸ä»¥ visual.*/vision.* ä½œä¸ºè§†è§‰åˆ†æ”¯
    prefixes = ("visual.", "vision.", "image_encoder.", "backbone.")
    return k.startswith(prefixes)

def _is_text_key(k):
    """åˆ¤å®šæ˜¯å¦ä¸ºæ–‡æœ¬ç¼–ç å™¨ç›¸å…³å‚æ•°"""
    # æ–‡æœ¬åˆ†æ”¯å¸¸è§é”®ï¼štransformer.* / token_embedding.* / positional_embedding / ln_final.* / text_projection
    if k.startswith(("text.", "transformer.", "token_embedding.")):
        return True
    if k in ("positional_embedding", "text_projection", "ln_final.weight", "ln_final.bias"):
        return True
    if k.startswith("ln_final."):
        return True
    return False

def _split_state_dict(state_dict):
    """æŒ‰è§†è§‰/æ–‡æœ¬æ‹†åˆ†æƒé‡"""
    img_sd = OrderedDict()
    txt_sd = OrderedDict()
    others = []

    for k, v in state_dict.items():
        ck = _strip_prefix(k)
        if _is_image_key(ck):
            img_sd[ck] = v
        elif _is_text_key(ck):
            txt_sd[ck] = v
        else:
            # ä¾‹å¦‚ logit_scale ç­‰å…¨å±€å‚æ•°ï¼Œè¿™é‡Œå¿½ç•¥
            others.append(ck)

    return img_sd, txt_sd, others

def convert_to_backbone_format(state_dict):
    """å°†æƒé‡keysä»visual.trunk.æ ¼å¼è½¬æ¢ä¸ºbackbone.æ ¼å¼
    
    ç›®æ ‡æ ¼å¼ç¤ºä¾‹:
    - backbone.stem_0
    - backbone.stem_1  
    - backbone.stages_0.blocks.0.gamma
    - backbone.stages_0.blocks.0.conv_dw.weight
    - backbone.stages_0.blocks.0.norm.weight
    - backbone.stages_0.blocks.0.mlp.fc1.weight
    
    Args:
        state_dict: åŸå§‹æƒé‡å­—å…¸
    
    Returns:
        è½¬æ¢åçš„æƒé‡å­—å…¸
    """
    converted_dict = OrderedDict()
    
    for key, tensor in state_dict.items():
        new_key = key
        
        # ç§»é™¤visualå‰ç¼€ï¼Œä¿ç•™trunkéƒ¨åˆ†è¿›è¡Œè½¬æ¢
        if new_key.startswith('visual.'):
            new_key = new_key[len('visual.'):]
        
        # å°†trunkå‰ç¼€æ›¿æ¢ä¸ºbackbone
        if new_key.startswith('trunk.'):
            new_key = new_key.replace('trunk.', 'backbone.', 1)
        
        # å¤„ç†stemå±‚ï¼štrunk.stem.0 -> backbone.stem_0
        if 'backbone.stem.' in new_key:
            new_key = new_key.replace('backbone.stem.', 'backbone.stem_')
        
        # å¤„ç†stageså±‚ï¼štrunk.stages.0 -> backbone.stages_0
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… backbone.stages.{æ•°å­—}
        new_key = re.sub(r'backbone\.stages\.(\d+)', r'backbone.stages_\1', new_key)
        
        # å¤„ç†downsampleå±‚ï¼šå¦‚æœå­˜åœ¨downsampleï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè°ƒæ•´
        # è¿™é‡Œå‡è®¾downsampleå±‚ä¹Ÿéµå¾ªç±»ä¼¼çš„å‘½åè§„åˆ™
        if 'downsample.' in new_key:
            # å¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´downsampleçš„æ˜ å°„è§„åˆ™
            pass
        
        converted_dict[new_key] = tensor
    
    return converted_dict

def save_converted_weights(state_dict, output_path, description=""):
    """ä¿å­˜è½¬æ¢åçš„æƒé‡æ–‡ä»¶
    
    Args:
        state_dict: è¦ä¿å­˜çš„æƒé‡å­—å…¸
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        description: æè¿°ä¿¡æ¯
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ä¿å­˜æƒé‡
        torch.save(state_dict, output_path)
        print(f"âœ… å·²ä¿å­˜{description}æƒé‡: {output_path}")
        print(f"   å‚æ•°å±‚æ•°: {len(state_dict)}ï¼Œå‚æ•°æ€»é‡: {_num_params(state_dict)}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªkeyä½œä¸ºç¤ºä¾‹
        print(f"   ç¤ºä¾‹keys: {list(state_dict.keys())[:5]}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æƒé‡æ–‡ä»¶å¤±è´¥ {output_path}: {e}")

def _guess_convnext_variant(weights):
    """ä»æƒé‡ä¸­çš„é…ç½®å°è¯•æ¨æ–­ ConvNeXt å˜ä½“ï¼Œè¿”å›æˆ‘ä»¬é¡¹ç›®ä¸­ä½¿ç”¨çš„å‘½ååç¼€ï¼š
    - base/convnext_base -> convnext_b
    - large/convnext_large -> convnext_l
    - small/convnext_small -> convnext_s
    - tiny/convnext_tiny -> convnext_t
    é»˜è®¤å›é€€ä¸º convnext_b
    """
    candidates = []
    if isinstance(weights, dict):
        for topk in ["model_cfg", "config", "cfg", "args"]:
            if topk in weights and isinstance(weights[topk], dict):
                candidates.append(weights[topk])
    # é€’å½’æŸ¥æ‰¾å­—ç¬¦ä¸²å€¼é‡ŒåŒ…å« convnext çš„çº¿ç´¢
    def _find_variant(d):
        try:
            for k, v in d.items():
                if isinstance(v, dict):
                    val = _find_variant(v)
                    if val:
                        return val
                elif isinstance(v, str):
                    s = v.lower()
                    if "convnext" in s:
                        return s
        except Exception:
            return None
        return None

    found = None
    for c in candidates:
        found = _find_variant(c)
        if found:
            break

    if not found:
        # è¿˜å¯ä»¥åœ¨ state_dict é”®åä¸ŠçŒœæµ‹ï¼ˆè‹¥è§†è§‰åˆ†æ”¯ä¸­å«æœ‰ convnext ç»“æ„ç‰¹å¾ï¼Œä¹Ÿè¾ƒå›°éš¾ï¼‰
        return "convnext_b"

    if "tiny" in found:
        return "convnext_t"
    if "small" in found or re.search(r"\bconvnext_s\b", found):
        return "convnext_s"
    if "large" in found or re.search(r"\bconvnext_l\b", found):
        return "convnext_l"
    # base/default
    return "convnext_b"

def _num_params(sd):
    try:
        return sum(int(v.numel()) for v in sd.values())
    except Exception:
        return 0

def main():
    """ä¸»å‡½æ•° - åŠ è½½æƒé‡å¹¶è½¬æ¢ä¸ºbackboneæ ¼å¼"""
    print("ğŸ” å¼€å§‹æƒé‡è½¬æ¢ä¸ºbackboneæ ¼å¼...")
    print("="*60)
    
    # åŠ è½½æƒé‡æ–‡ä»¶
    print("ğŸ“‚ åŠ è½½æƒé‡æ–‡ä»¶:")
    weights = load_weights(weight_path)
    if weights is None:
        return

    # æå– state_dict
    state_dict = _extract_state_dict(weights)
    if state_dict is None:
        return

    # æ‹†åˆ†ä¸º image/text ä¸¤éƒ¨åˆ†
    print("âœ‚ï¸  æ‹†åˆ†æƒé‡ä¸º Image Encoder ä¸ Text Encoder ...")
    img_sd, txt_sd, others = _split_state_dict(state_dict)
    print(f"ğŸ–¼ï¸  Image Encoder å‚æ•°å±‚æ•°: {len(img_sd)}ï¼Œå‚æ•°æ€»é‡: {_num_params(img_sd)}")
    print(f"ğŸ“ Text Encoder  å‚æ•°å±‚æ•°: {len(txt_sd)}ï¼Œå‚æ•°æ€»é‡: {_num_params(txt_sd)}")
    if others:
        print(f"â„¹ï¸  å…¶ä½™æœªå½’ç±»å‚æ•°ï¼ˆå°†è¢«å¿½ç•¥ï¼‰æ•°é‡: {len(others)}ï¼Œä¾‹å¦‚: {others[:5]} ...")

    # è½¬æ¢Image Encoderä¸ºbackboneæ ¼å¼
    print("\nğŸ”„ è½¬æ¢Image Encoderä¸ºbackboneæ ¼å¼...")
    converted_img_sd = convert_to_backbone_format(img_sd)
    
    # æ¨æ–­ ConvNeXt å˜ä½“ç”¨äºå‘½å
    variant = _guess_convnext_variant(weights)
    out_dir = "/opt/DM/OCT/CLIP_Code/CLIP-EBC/models/clip/_clip/weights"
    os.makedirs(out_dir, exist_ok=True)
    
    # ä¿å­˜åŸå§‹æ ¼å¼çš„æƒé‡
    img_out = os.path.join(out_dir, f"clip_image_encoder_{variant}.pth")
    txt_out = os.path.join(out_dir, f"clip_text_encoder_{variant}.pth")
    
    # ä¿å­˜backboneæ ¼å¼çš„æƒé‡
    backbone_img_out = os.path.join(out_dir, f"backbone_convnext_{variant}.pth")
    
    print("\nğŸ’¾ ä¿å­˜æƒé‡æ–‡ä»¶:")
    
    # ä¿å­˜åŸå§‹æ ¼å¼
    save_converted_weights(img_sd, img_out, "åŸå§‹Image Encoder")
    save_converted_weights(txt_sd, txt_out, "Text Encoder")
    
    # ä¿å­˜backboneæ ¼å¼
    save_converted_weights(converted_img_sd, backbone_img_out, "backboneæ ¼å¼")
    
    print("\nğŸ“Š è½¬æ¢å¯¹æ¯”:")
    print(f"åŸå§‹Image Encoder keysç¤ºä¾‹: {list(img_sd.keys())[:3]}")
    print(f"backboneæ ¼å¼keysç¤ºä¾‹: {list(converted_img_sd.keys())[:3]}")
    
    # éªŒè¯è½¬æ¢ç»“æœ
    print("\nğŸ” è½¬æ¢ç»“æœéªŒè¯:")
    backbone_keys = list(converted_img_sd.keys())
    stem_keys = [k for k in backbone_keys if 'stem_' in k]
    stage_keys = [k for k in backbone_keys if 'stages_' in k][:5]
    
    print(f"Stemå±‚keys: {stem_keys}")
    print(f"Stageå±‚keysç¤ºä¾‹: {stage_keys}")
    
    print("="*60)
    print("ğŸ‰ æƒé‡è½¬æ¢å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {out_dir}")
    print(f"ğŸ“„ åŸå§‹æ ¼å¼: {os.path.basename(img_out)}, {os.path.basename(txt_out)}")
    print(f"ğŸ“„ backboneæ ¼å¼: {os.path.basename(backbone_img_out)}")

if __name__ == "__main__":
    main()